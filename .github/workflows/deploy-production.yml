name: Deploy to Production

on:
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      skip_tests:
        description: 'Skip tests (emergency deploy)'
        required: false
        default: 'false'

jobs:
  test:
    name: Run Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.skip_tests != 'true'
    
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: connect_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    env:
      DATABASE_URL: postgresql://postgres:postgres@localhost:6432/connect_test
      DIRECT_URL: postgresql://postgres:postgres@localhost:6432/connect_test
      REDIS_CACHE_URL: redis://localhost:6379/0
      NODE_ENV: test
      ENCRYPTION_KEY: 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef
      NEXTAUTH_SECRET: test_nextauth_secret_for_ci_testing_only
      NEXTAUTH_URL: http://localhost:3000
      JWT_SECRET: test_jwt_secret_for_ci_testing_only
      NAVER_CLIENT_ID: test_naver_client_id_for_ci
      NAVER_CLIENT_SECRET: test_naver_client_secret_for_ci
      KAKAO_CLIENT_ID: test_kakao_client_id_for_ci
      KAKAO_CLIENT_SECRET: test_kakao_client_secret_for_ci
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20.x'
          cache: 'npm'
      
      - name: Install dependencies
        run: npm ci
      
      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client
      
      - name: Wait for PostgreSQL
        run: |
          echo "Waiting for PostgreSQL to be ready..."
          for i in {1..30}; do
            if pg_isready -h localhost -p 6432 -U postgres; then
              echo "‚úì PostgreSQL is ready!"
              break
            fi
            echo "Attempt $i/30: PostgreSQL not ready yet..."
            sleep 2
          done
          
          # Verify connection
          PGPASSWORD=postgres psql -h localhost -p 6432 -U postgres -d connect_test -c "SELECT version();"
      
      - name: Setup test database
        run: |
          echo "Generating Prisma client..."
          npx prisma generate
          
          echo "Pushing database schema..."
          npx prisma db push --skip-generate --accept-data-loss
          
          echo "Verifying database schema..."
          PGPASSWORD=postgres psql -h localhost -p 6432 -U postgres -d connect_test -c "\dt"
          
          echo "‚úì Database setup complete"
      
      - name: Verify environment
        run: |
          echo "DATABASE_URL: $DATABASE_URL"
          echo "NODE_ENV: $NODE_ENV"
          echo "Prisma client location:"
          ls -la node_modules/.prisma/client/ || echo "Prisma client not found"
      
      - name: Run tests
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:6432/connect_test
          DIRECT_URL: postgresql://postgres:postgres@localhost:6432/connect_test
        run: npm test -- --passWithNoTests
      
      - name: Run linter
        run: npm run lint

  build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: test
    if: always() && (needs.test.result == 'success' || needs.test.result == 'skipped')

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Free disk space on runner
        run: |
          echo "üßπ Freeing disk space on GitHub Actions runner..."
          echo "Before cleanup:"
          df -h /

          # Remove unnecessary large packages
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc
          sudo rm -rf /opt/hostedtoolcache/CodeQL

          # Clean Docker system
          docker system prune -af --volumes || true
          docker builder prune -af || true

          echo ""
          echo "After cleanup:"
          df -h /
          echo "‚úÖ Disk space freed"

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build production image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile.production
          push: false
          tags: connect:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          outputs: type=docker,dest=/tmp/connect-image.tar

      - name: Build scraper image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile.scraper
          push: false
          tags: connect-scraper:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          outputs: type=docker,dest=/tmp/connect-scraper-image.tar

      - name: Verify scripts directory in images
        run: |
          echo "üîç Verifying Docker images contain required files..."

          # ========== Scraper Image ==========
          docker load --input /tmp/connect-scraper-image.tar

          # Check 1: Build requirements (scripts directory for ad-hoc operations)
          echo "Checking scraper build requirements..."
          if ! docker run --rm --entrypoint ls connect-scraper:${{ github.sha }} -la /app/scripts/ >/dev/null 2>&1; then
            echo "‚ùå Missing /app/scripts/ in scraper image"
            echo "   Required for: ad-hoc operations (e.g., scrape-ntis-historical.ts)"
            echo "   Fix: Verify Dockerfile.scraper line 27 contains 'COPY scripts ./scripts'"
            exit 1
          fi
          echo "‚úÖ /app/scripts/ exists in scraper image"

          # Check 2: Runtime requirements
          echo "Checking scraper runtime requirements..."
          docker run --rm --entrypoint sh connect-scraper:${{ github.sha }} -c '
            if [ ! -f /app/lib/scraping/worker.ts ]; then
              echo "‚ùå Missing /app/lib/scraping/worker.ts (entrypoint execution target)"
              exit 1
            fi
            if [ ! -f /app/docker-entrypoint-scraper.sh ]; then
              echo "‚ùå Missing /app/docker-entrypoint-scraper.sh"
              exit 1
            fi
            echo "‚úÖ Runtime files present: worker.ts, entrypoint script"
          '

          # Check 3: Image metadata
          echo "Verifying scraper image metadata..."
          docker image inspect connect-scraper:${{ github.sha }} \
            --format 'WorkingDir={{.Config.WorkingDir}} Entrypoint={{json .Config.Entrypoint}}'

          # ========== App Image ==========
          docker load --input /tmp/connect-image.tar

          echo "Checking app build requirements..."
          if ! docker run --rm --entrypoint ls connect:${{ github.sha }} -la /app/scripts/ >/dev/null 2>&1; then
            echo "‚ùå Missing /app/scripts/ in app image"
            echo "   Required for: maintenance scripts and ad-hoc operations"
            echo "   Fix: Verify Dockerfile.production line 94 contains scripts COPY"
            exit 1
          fi
          echo "‚úÖ /app/scripts/ exists in app image"

          echo "Checking app runtime requirements..."
          docker run --rm --entrypoint sh connect:${{ github.sha }} -c '
            if [ ! -f /app/docker-entrypoint.sh ]; then
              echo "‚ùå Missing /app/docker-entrypoint.sh"
              exit 1
            fi
            if [ ! -f /app/server.js ]; then
              echo "‚ùå Missing /app/server.js (Next.js standalone server)"
              exit 1
            fi
            echo "‚úÖ Runtime files present: server.js, entrypoint script"
          '

          echo "Verifying app image metadata..."
          docker image inspect connect:${{ github.sha }} \
            --format 'WorkingDir={{.Config.WorkingDir}} Entrypoint={{json .Config.Entrypoint}}'

          echo ""
          echo "‚úÖ All images verified successfully"
          echo "   - Build requirements: /app/scripts/ present"
          echo "   - Runtime requirements: entrypoints and execution targets present"
          echo "   - Image metadata: validated"

      - name: Upload app image artifact
        uses: actions/upload-artifact@v4
        with:
          name: docker-image
          path: /tmp/connect-image.tar
          retention-days: 1

      - name: Upload scraper image artifact
        uses: actions/upload-artifact@v4
        with:
          name: docker-scraper-image
          path: /tmp/connect-scraper-image.tar
          retention-days: 1

  deploy:
    name: Deploy to Production Server
    runs-on: ubuntu-latest
    needs: build
    environment:
      name: production
      url: https://connectplt.kr
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download app image artifact
        uses: actions/download-artifact@v4
        with:
          name: docker-image
          path: /tmp

      - name: Download scraper image artifact
        uses: actions/download-artifact@v4
        with:
          name: docker-scraper-image
          path: /tmp

      - name: Load Docker images
        run: |
          docker load --input /tmp/connect-image.tar
          docker load --input /tmp/connect-scraper-image.tar
      
      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.PRODUCTION_SERVER_SSH_KEY }}" > ~/.ssh/deploy_key
          chmod 600 ~/.ssh/deploy_key
          ssh-keyscan -H ${{ secrets.PRODUCTION_SERVER_IP }} >> ~/.ssh/known_hosts

      - name: Copy configuration files to production
        run: |
          echo "Copying docker-compose.production.yml to production server..."
          scp -i ~/.ssh/deploy_key \
          docker-compose.production.yml \
          ${{ secrets.PRODUCTION_SERVER_USER }}@${{ secrets.PRODUCTION_SERVER_IP }}:/opt/connect/

          echo "‚úì Configuration files copied successfully"

      - name: Sync source files to production
        run: |
          echo "Syncing source files to production host..."
          scp -i ~/.ssh/deploy_key -r \
            scripts/ \
            ${{ secrets.PRODUCTION_SERVER_USER }}@${{ secrets.PRODUCTION_SERVER_IP }}:/opt/connect/

          echo "‚úì Source files synchronized"

      - name: Generate .env.production from GitHub Secrets
        run: |
          cat > .env.production << 'EOF'
          # ============================================
          # Connect Platform - Production Environment
          # ============================================
          # Auto-generated from GitHub Secrets during deployment
          # Last Updated: $(date +"%B %d, %Y")
          # DO NOT EDIT MANUALLY - This file is regenerated on every deployment

          # Node Environment
          NODE_ENV=production

          # ============================================
          # Database Configuration
          # ============================================
          DB_PASSWORD=${{ secrets.DB_PASSWORD }}
          DATABASE_URL=postgresql://connect:${DB_PASSWORD}@pgbouncer:6432/connect?schema=public&pool_timeout=30&connection_limit=50

          # ============================================
          # Redis Configuration
          # ============================================
          REDIS_CACHE_URL=redis://redis-cache:6379/0
          REDIS_QUEUE_URL=redis://redis-queue:6380/0

          # ============================================
          # Authentication & Security
          # ============================================
          JWT_SECRET=${{ secrets.JWT_SECRET }}
          NEXTAUTH_SECRET=${{ secrets.NEXTAUTH_SECRET }}
          NEXTAUTH_URL=https://connectplt.kr

          # ============================================
          # Data Encryption (PIPA Compliance - REQUIRED)
          # ============================================
          ENCRYPTION_KEY=${{ secrets.ENCRYPTION_KEY }}
          ENCRYPTION_KEY_CREATED=2025-10-10
          ENCRYPTION_KEY_NEXT_ROTATION=2026-01-10

          # ============================================
          # OAuth Providers (Korean Services)
          # ============================================
          KAKAO_CLIENT_ID=${{ secrets.KAKAO_CLIENT_ID }}
          KAKAO_CLIENT_SECRET=${{ secrets.KAKAO_CLIENT_SECRET }}
          NAVER_CLIENT_ID=${{ secrets.NAVER_CLIENT_ID }}
          NAVER_CLIENT_SECRET=${{ secrets.NAVER_CLIENT_SECRET }}

          # ============================================
          # Email Configuration - AWS SES (Seoul Region)
          # ============================================
          SMTP_HOST=email-smtp.ap-northeast-2.amazonaws.com
          SMTP_PORT=587
          SMTP_SECURE=false
          SMTP_USER=${{ secrets.SMTP_USER }}
          SMTP_PASSWORD=${{ secrets.SMTP_PASSWORD }}
          SMTP_FROM_EMAIL=support@connectplt.kr
          SMTP_FROM_NAME=Connect
          SMTP_REPLY_TO=support@connectplt.kr
          ADMIN_EMAIL=support@connectplt.kr

          # ============================================
          # NTIS API (National Science & Technology Information Service)
          # ============================================
          NTIS_API_KEY=${{ secrets.NTIS_API_KEY }}

          # ============================================
          # Anthropic Claude AI
          # ============================================
          ANTHROPIC_API_KEY=${{ secrets.ANTHROPIC_API_KEY }}
          ANTHROPIC_MODEL=claude-sonnet-4-5-20250929
          ANTHROPIC_MAX_TOKENS=4096
          ANTHROPIC_TEMPERATURE=0.7

          # AI Rate Limiting & Budget
          AI_RATE_LIMIT_PER_MINUTE=50
          AI_DAILY_BUDGET_KRW=50000

          # ============================================
          # Payment Integration (Toss Payments)
          # ============================================
          TOSS_CLIENT_KEY=${{ secrets.TOSS_CLIENT_KEY }}
          TOSS_SECRET_KEY=${{ secrets.TOSS_SECRET_KEY }}
          TOSS_TEST_MODE=${{ secrets.TOSS_TEST_MODE }}

          # ============================================
          # Monitoring & Error Tracking
          # ============================================
          SENTRY_DSN=
          GRAFANA_PASSWORD=${{ secrets.GRAFANA_PASSWORD }}

          # ============================================
          # Scraping Configuration
          # ============================================
          SCRAPER_CONCURRENCY=2
          SCRAPER_TIMEOUT=30000
          SCRAPER_USER_AGENT=Mozilla/5.0 (compatible; ConnectBot/1.0; +https://connectplt.kr)

          # Agency URLs
          IITP_URL=https://www.iitp.kr
          KEIT_URL=https://www.keit.re.kr
          TIPA_URL=https://www.tipa.or.kr
          KIMST_URL=https://www.kimst.re.kr

          # Rate Limiting
          RATE_LIMIT_PER_MINUTE=10

          # Scraping Schedule
          SCRAPING_SCHEDULE_NORMAL=0 9,15 * * *
          SCRAPING_SCHEDULE_PEAK=0 9,12,15,18 1-3 * *

          # NTIS Date Filtering (Daily Scraper)
          NTIS_SCRAPING_DAYS_BACK=2

          # ============================================
          # Cron Job Security
          # ============================================
          CRON_SECRET_TOKEN=${{ secrets.CRON_SECRET_TOKEN }}

          # ============================================
          # Application Settings
          # ============================================
          NEXT_PUBLIC_APP_URL=https://connectplt.kr
          DOMAIN=connectplt.kr

          # ============================================
          # Feature Flags
          # ============================================
          ENABLE_ANALYTICS=true
          ENABLE_ERROR_TRACKING=false
          MAINTENANCE_MODE=false
          EOF

          echo "‚úì .env.production generated from GitHub Secrets (149 lines)"

      - name: Transfer .env.production to production server
        run: |
          echo "Transferring .env.production to production server..."
          scp -i ~/.ssh/deploy_key \
            .env.production \
            ${{ secrets.PRODUCTION_SERVER_USER }}@${{ secrets.PRODUCTION_SERVER_IP }}:/opt/connect/.env.production

          # Set secure permissions
          ssh -i ~/.ssh/deploy_key \
            ${{ secrets.PRODUCTION_SERVER_USER }}@${{ secrets.PRODUCTION_SERVER_IP }} \
            'chmod 600 /opt/connect/.env.production'

          echo "‚úì Environment file transferred and secured"

      - name: Create symlink for Docker Compose compatibility
        run: |
          echo "Creating symlink .env ‚Üí .env.production..."
          ssh -i ~/.ssh/deploy_key \
            ${{ secrets.PRODUCTION_SERVER_USER }}@${{ secrets.PRODUCTION_SERVER_IP }} << 'EOF'
            cd /opt/connect

            # Remove old .env if it's a regular file
            if [ -f .env ] && [ ! -L .env ]; then
              echo "Backing up old .env to .env.backup-manual-$(date +%Y%m%d-%H%M%S)"
              mv .env .env.backup-manual-$(date +%Y%m%d-%H%M%S)
            fi

            # Remove old symlink if it exists
            if [ -L .env ]; then
              rm .env
            fi

            # Create new symlink
            ln -s .env.production .env

            # Verify symlink
            if [ -L .env ]; then
              echo "‚úì Symlink created: .env ‚Üí .env.production"
              ls -la .env
            else
              echo "‚ùå Failed to create symlink"
              exit 1
            fi
          EOF

      - name: Free disk space on production server
        run: |
          echo "üßπ Freeing disk space on production server BEFORE deployment..."
          ssh -i ~/.ssh/deploy_key \
          ${{ secrets.PRODUCTION_SERVER_USER }}@${{ secrets.PRODUCTION_SERVER_IP }} << 'EOF'
            echo "Before cleanup:"
            df -h /

            # Remove dangling images (not tagged or used by containers)
            echo "Removing dangling images..."
            docker image prune -f

            # Remove images older than 24 hours (except currently running)
            echo "Removing old images..."
            docker image prune -a --filter "until=24h" -f || true

            # Clean build cache
            echo "Cleaning build cache..."
            docker builder prune -f --filter "until=12h" || true

            # Remove stopped containers older than 1 hour
            echo "Removing old stopped containers..."
            docker container prune -f --filter "until=1h" || true

            echo ""
            echo "After cleanup:"
            df -h /
            echo "‚úÖ Production server disk space freed"
          EOF

      - name: Copy images to production
        run: |
          echo "Transferring app image..."
          docker save connect:${{ github.sha }} | gzip | \
          ssh -i ~/.ssh/deploy_key \
          ${{ secrets.PRODUCTION_SERVER_USER }}@${{ secrets.PRODUCTION_SERVER_IP }} \
          'gunzip | docker load'

          echo "Transferring scraper image..."
          docker save connect-scraper:${{ github.sha }} | gzip | \
          ssh -i ~/.ssh/deploy_key \
          ${{ secrets.PRODUCTION_SERVER_USER }}@${{ secrets.PRODUCTION_SERVER_IP }} \
          'gunzip | docker load'
      
      - name: Deploy to production
        run: |
          ssh -i ~/.ssh/deploy_key \
          ${{ secrets.PRODUCTION_SERVER_USER }}@${{ secrets.PRODUCTION_SERVER_IP }} << 'EOF'
            cd /opt/connect

            # Tag new images
            docker tag connect:${{ github.sha }} connect:latest
            docker tag connect-scraper:${{ github.sha }} connect-scraper:latest

            # Deploy using docker-compose (migrations run automatically inside containers)
            export COMPOSE_FILE=docker-compose.production.yml

            # Blue-Green deployment for app instances
            if docker ps | grep -q connect_app1; then
              echo "üîÑ Deploying to app2 first (blue-green deployment)..."
              docker-compose --env-file .env.production up -d app2

              # Wait for health check (migrations run automatically inside container)
              echo "‚è≥ Waiting for app2 to be healthy (including migrations)..."
              timeout 180 sh -c 'until curl -sf http://172.25.0.22:3002/api/health; do sleep 5; done'

              # Switch traffic and update app1
              echo "‚úÖ App2 healthy. Updating app1..."
              docker-compose --env-file .env.production stop app1
              docker-compose --env-file .env.production up -d app1

              # Wait for app1
              echo "‚è≥ Waiting for app1 to be healthy..."
              timeout 180 sh -c 'until curl -sf http://172.25.0.21:3001/api/health; do sleep 5; done'

              echo "‚úÖ Both app instances healthy!"
            else
              echo "üÜï First deployment - starting both app instances..."
              docker-compose --env-file .env.production up -d app1 app2

              # Wait for both to be healthy
              timeout 180 sh -c 'until curl -sf http://172.25.0.21:3001/api/health; do sleep 5; done'
              timeout 180 sh -c 'until curl -sf http://172.25.0.22:3002/api/health; do sleep 5; done'
              echo "‚úÖ Both app instances healthy!"
            fi

            # Deploy scraper (with fresh code and health checks)
            echo "üîÑ Deploying scraper..."
            docker-compose --env-file .env.production up -d scraper

            # Wait for scraper to be healthy
            echo "‚è≥ Waiting for scraper to be healthy..."
            for i in {1..30}; do
              if docker inspect connect_scraper --format='{{.State.Health.Status}}' 2>/dev/null | grep -q "healthy"; then
                echo "‚úÖ Scraper is healthy!"
                break
              fi
              echo "Attempt $i/30: Scraper not healthy yet..."
              sleep 5
            done

            # Verify scraper is running
            if docker ps | grep -q connect_scraper; then
              echo "‚úÖ Scraper is running"
            else
              echo "‚ùå Scraper failed to start"
              docker logs connect_scraper --tail 50
              exit 1
            fi

            echo "üéâ Deployment complete! (App1, App2, Scraper)"
          EOF
      
      - name: Run health checks
        run: |
          echo "‚è≥ Waiting for containers to be fully ready..."
          sleep 10

          # Check app1 on its direct port (allow 3 minutes for migrations + startup)
          for i in {1..36}; do
            if curl -f http://59.21.170.6:3001/api/health; then
              echo "‚úÖ App1 health check passed!"
              exit 0
            fi
            echo "Attempt $i/36: Health check failed, retrying..."
            sleep 5
          done

          echo "‚ùå Health checks failed after 3 minutes!"
          exit 1
      
      - name: Notify on success
        if: success()
        run: |
          echo "‚úÖ Deployment successful!"
          echo "Version: ${{ github.sha }}"
          echo "Deployed by: ${{ github.actor }}"

      - name: Cleanup old Docker images
        if: success()
        run: |
          ssh -i ~/.ssh/deploy_key \
          ${{ secrets.PRODUCTION_SERVER_USER }}@${{ secrets.PRODUCTION_SERVER_IP }} << 'EOF'
          echo "üßπ Cleaning Docker images older than 72 hours..."
          docker image prune -a --filter "until=72h" -f

          echo "üßπ Cleaning build cache older than 24 hours..."
          docker builder prune -f --filter "until=24h"

          echo ""
          echo "‚úÖ Cleanup complete. Current Docker disk usage:"
          docker system df
          EOF

      - name: Rollback on failure
        if: failure()
        run: |
          echo "‚ùå Deployment failed! Rolling back..."
          ssh -i ~/.ssh/deploy_key \
          ${{ secrets.PRODUCTION_SERVER_USER }}@${{ secrets.PRODUCTION_SERVER_IP }} \
          'cd /opt/connect && docker-compose --env-file .env.production restart app1 app2 scraper'

